{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64881fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db import execute_query\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import functools\n",
    "from scipy.optimize import brentq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4496bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "alpha = 0.75\n",
    "threshold = 0.4\n",
    "\n",
    "def run_trial():\n",
    "    query = f\"\"\"\n",
    "    select audio_id, background_modifier_id, audio_info_id\n",
    "    from audio_data \n",
    "    \"\"\"\n",
    "\n",
    "    class Audio:\n",
    "        def __init__(self, row):\n",
    "            self.audio_id = row[0]\n",
    "            self.background_modifier = row[1]\n",
    "            self.info_id = row[2]\n",
    "\n",
    "    data = execute_query(query)\n",
    "    audio_ids = np.array([Audio(d) for d in data])\n",
    "    background_modifier_ids = np.array([d[1] for d in data])\n",
    "    audio_info_ids = np.array([d[2] for d in data])\n",
    "\n",
    "    labels = np.array(list(zip(background_modifier_ids, audio_info_ids)))\n",
    "    audio_train, audio_test, _, _ = train_test_split(audio_ids, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "    calibration_ids = [audio.audio_id for audio in audio_train]\n",
    "    calibration_features = [audio.background_modifier for audio in audio_train]\n",
    "\n",
    "    lst = [audio for audio in audio_test if audio.background_modifier == 2]\n",
    "    validation_ids = [audio.audio_id for audio in lst]\n",
    "    validation_features = [audio.background_modifier for audio in lst]\n",
    "    \n",
    "    def create_mapping(data):\n",
    "        mapping = {}\n",
    "        idx = 0\n",
    "        for item in data:\n",
    "            audio_id = item[0]\n",
    "            if audio_id not in mapping:\n",
    "                mapping[audio_id] = idx\n",
    "                idx += 1\n",
    "        return mapping\n",
    "\n",
    "    n = len(calibration_ids)\n",
    "    calibration_predictions = [[] for _ in range(n)]\n",
    "    calibration_confidence_scores = [[] for _ in range(n)]\n",
    "    calibration_word_error_rates = [[] for _ in range(n)]\n",
    "    calibration_audio_info_ids = [[] for _ in range(n)]\n",
    "    c_background_modifier_ids = [[] for _ in range(n)]\n",
    "\n",
    "    query = f\"\"\"\n",
    "    select Audio_ID, Prediction, Confidence_Score, Word_Error_Rate, Audio_Info_ID, Background_Modifier_ID\n",
    "    from audio_data \n",
    "    natural join audio_predictions\n",
    "    where audio_id in {tuple(calibration_ids)}\n",
    "    \"\"\"\n",
    "    rows = execute_query(query)\n",
    "    mapping = create_mapping(rows)\n",
    "\n",
    "    for row in rows:\n",
    "        idx = mapping[row[0]]\n",
    "        calibration_predictions[idx].append(row[1])\n",
    "        calibration_confidence_scores[idx].append(row[2])\n",
    "        calibration_word_error_rates[idx].append(row[3])\n",
    "        calibration_audio_info_ids[idx].append(row[4])\n",
    "        c_background_modifier_ids[idx].append(row[5])\n",
    "\n",
    "    n = len(validation_ids)\n",
    "    validation_predictions = [[] for _ in range(n)]\n",
    "    validation_confidence_scores = [[] for _ in range(n)]\n",
    "    validation_word_error_rates = [[] for _ in range(n)]\n",
    "    validation_audio_info_ids = [[] for _ in range(n)]\n",
    "    v_background_modifier_ids = [[] for _ in range(n)]\n",
    "\n",
    "    query = f\"\"\"\n",
    "    select Audio_ID, Prediction, Confidence_Score, Word_Error_Rate, Audio_Info_ID, Background_Modifier_ID\n",
    "    from audio_data \n",
    "    natural join audio_predictions\n",
    "    where audio_id in {tuple(validation_ids)}\n",
    "    \"\"\"\n",
    "    rows = execute_query(query)\n",
    "    mapping = create_mapping(rows)\n",
    "\n",
    "    for row in rows:\n",
    "        idx = mapping[row[0]]\n",
    "        validation_predictions[idx].append(row[1])\n",
    "        validation_confidence_scores[idx].append(row[2])\n",
    "        validation_word_error_rates[idx].append(row[3])\n",
    "        validation_audio_info_ids[idx].append(row[4])\n",
    "        v_background_modifier_ids[idx].append(row[5])\n",
    "\n",
    "    def flat(input_list):\n",
    "        flattened_list = []\n",
    "        for sublist in input_list:\n",
    "            if isinstance(sublist, list):\n",
    "                if sublist:  # Check if the sublist is not empty\n",
    "                    flattened_list.append(sublist[0])\n",
    "            else:\n",
    "                flattened_list.append(sublist)\n",
    "        return flattened_list\n",
    "\n",
    "    calibration_features = flat(c_background_modifier_ids)\n",
    "    validation_features = flat(v_background_modifier_ids)\n",
    "\n",
    "    def compute_weight_schedule(calibration_features, validation_features):\n",
    "        X, y = calibration_features + validation_features, [0] * len(calibration_features) + [1] * len(validation_features)\n",
    "        X, y = np.array(X).reshape(-1, 1), np.array(y)\n",
    "\n",
    "        binary_classifier = RandomForestClassifier()\n",
    "        binary_classifier.fit(X, y)\n",
    "\n",
    "        weight_fn = {}\n",
    "        feature_set = set(calibration_features) | set(validation_features)\n",
    "        for feature in feature_set:\n",
    "            probabilities = binary_classifier.predict_proba([[feature]])[0]\n",
    "            weight_fn[feature] = probabilities[1] / (1 - probabilities[1])\n",
    "\n",
    "        weight_schedule = [weight_fn[feature] for feature in calibration_features]\n",
    "        return weight_schedule\n",
    "\n",
    "    weights = compute_weight_schedule(calibration_features, validation_features)\n",
    "\n",
    "    def c_lam(lam, smx):\n",
    "        \"\"\"Compute prediction set indexes using lambda\"\"\"\n",
    "        prefix_sums = np.cumsum(smx, axis=1)\n",
    "        threshold_indexes = np.zeros(prefix_sums.shape[0], dtype=int)\n",
    "        for idx, row in enumerate(prefix_sums):\n",
    "            threshold_idx = np.argmax(row >= lam)\n",
    "            threshold_indexes[idx] = threshold_idx if row[threshold_idx] >= lam else row.shape[0] - 1\n",
    "        return threshold_indexes\n",
    "\n",
    "    def loss(wers, wer_target):\n",
    "        \"\"\"Compute array of losses\"\"\"\n",
    "        return np.array([int(np.all(w >= wer_target)) for w in wers])\n",
    "\n",
    "    def losses(lam, smx, wers, wer_target, debug=False):\n",
    "        \"\"\"Compute array of losses given Lambda, also compute weight schedule\"\"\"\n",
    "        idxs = c_lam(lam, smx)\n",
    "        prediction_wers = []\n",
    "        for idx, threshold_idx in enumerate(idxs):\n",
    "            prediction_wers.append(wers[idx][:threshold_idx+1])\n",
    "            \n",
    "        if debug:\n",
    "            total_length = 0\n",
    "            for prediction_wer in prediction_wers:\n",
    "                total_length += len(prediction_wer)\n",
    "            print(\"Mean set size\", total_length / len(prediction_wers))\n",
    "\n",
    "        return loss(prediction_wers, wer_target)\n",
    "\n",
    "    def conformal_risk_control(lam, smx, wers, wer_target, weight_schedule):\n",
    "        n = smx.shape[0]\n",
    "        loss_values = losses(lam, smx, wers, wer_target)\n",
    "        \n",
    "        weighted_sum = 0\n",
    "        for idx, loss in enumerate(loss_values):\n",
    "            weighted_sum += weight_schedule[idx] * loss\n",
    "        \n",
    "        return weighted_sum / sum(weight_schedule) - ((n+1)/n*alpha - 1/(n+1))\n",
    "\n",
    "    def compute_lamhat(\n",
    "            confidence_scores, \n",
    "            word_error_rates, \n",
    "            wer_target, \n",
    "            weight_schedule):\n",
    "        \"\"\"Search for value of lambda that controls the WER\"\"\"\n",
    "        crc_partial = functools.partial(\n",
    "            conformal_risk_control, smx=confidence_scores, wers=word_error_rates, \n",
    "            wer_target=wer_target, weight_schedule=weight_schedule)\n",
    "        \n",
    "        try:\n",
    "            return brentq(crc_partial, 0, 1)\n",
    "        except ValueError as e:\n",
    "            if crc_partial(0) > 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "    lamhat = compute_lamhat(np.array(calibration_confidence_scores), np.array(calibration_word_error_rates), threshold, weights)\n",
    "    ls = losses(lamhat, np.array(validation_confidence_scores), np.array(validation_word_error_rates), threshold, debug=True)\n",
    "    return 1 - ls.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 5\n",
    "results = [run_trial() for _ in range(R)]\n",
    "C_hat = sum(results)/len(results)\n",
    "C_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp-snips",
   "language": "python",
   "name": "cp-snips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
